Singularity> python main.py --mode analyze
Using device: cuda
Loading model: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards: 100%|███████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.02it/s]
Running analysis on prompt: 'The history of artificial intelligence began in an...'
KV cache structure: 28 layers, 8 heads, 18 sequence length, 128 head dimensions

Model architecture:
  model_type: llama
  num_layers: 28
  num_heads: 24
  hidden_size: 3072
  head_dim: 128
  vocab_size: 128256
  total_params: 3212749824

Running layer analysis...
Running head analysis...
Running dimension analysis...
Running head consistency analysis...

Generating visualizations...

=== KV Cache Analysis Summary ===
Overall Key Sparsity: 0.1733
Overall Value Sparsity: 0.1902

Most Prunable Layers:
  Layer 22: Key Sparsity=0.2005, Value Sparsity=0.1350
  Layer 21: Key Sparsity=0.1937, Value Sparsity=0.1464
  Layer 25: Key Sparsity=0.1883, Value Sparsity=0.1269

Most Consistently Sparse Heads:
  Head 3: Mean Key Sparsity=0.1844 ± 0.0283
  Head 5: Mean Key Sparsity=0.1777 ± 0.0357
  Head 1: Mean Key Sparsity=0.1766 ± 0.0278

Top 5 Specific Pruning Targets:
  Layer 0, Head 3, Dimension 103: K-Sparsity=0.8333, V-Sparsity=0.6111
  Layer 15, Head 6, Dimension 75: K-Sparsity=0.8333, V-Sparsity=0.2778
  Layer 15, Head 6, Dimension 9: K-Sparsity=0.8333, V-Sparsity=0.1667
  Layer 15, Head 6, Dimension 71: K-Sparsity=0.8333, V-Sparsity=0.1111
  Layer 15, Head 6, Dimension 3: K-Sparsity=0.8333, V-Sparsity=0.0556

Pruning Potential: 3557/57344 parameters (6.20%)

Done!
Total execution time: 10.17 seconds